<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jose Abell's Research Blog</title><link>http://www.joseabell.com/</link><description></description><atom:link href="http://www.joseabell.com/feeds/jose-abell.rss.xml" rel="self"></atom:link><lastBuildDate>Thu, 29 Jan 2015 16:57:28 -0800</lastBuildDate><item><title>CS261 - HW # 0 : Describe a parallel application</title><link>http://www.joseabell.com/cs261-hw-0-describe-a-parallel-application.html</link><description>&lt;p&gt;by: José&amp;nbsp;Abell&lt;/p&gt;
&lt;h1 id="biography"&gt;Biography&lt;/h1&gt;
&lt;p&gt;I am a PhD student at UCDavis &lt;a href="http://sokocalo.engr.ucdavis.edu/~jeremic/"&gt;CompGeoMech&lt;/a&gt; since Sept 2011, working on our in-house high-performance &lt;span class="caps"&gt;FEM&lt;/span&gt; simulation system. I come from Chile, where I did my undergraduate studies in structural engineering, leading to a professional degree in the specialty, at Pontificia Universidad Católica de Chile. Then, I proceeded to do my &lt;span class="caps"&gt;MS&lt;/span&gt; at the same institution in Civil Engineering. My &lt;span class="caps"&gt;MS&lt;/span&gt; thesis was more focused into the hazard part of earthquake&amp;nbsp;engineering. &lt;/p&gt;
&lt;p&gt;My current research focuses on high-performance Earthquake-Soil-Structure interaction (&lt;span class="caps"&gt;ESSI&lt;/span&gt;) simulation. In a nutshell, it consists in modeling the effect of earthquake on structures from the earthquake source rupture process all the way up to a structure and its contents. The idea is to find out how and when a more accurate &lt;span class="caps"&gt;ESSI&lt;/span&gt; simulation yields different results from what is currently done in practice, and if this leads to safer and more economical&amp;nbsp;designs. &lt;/p&gt;
&lt;p&gt;Parallel, high-performance computing is an enabling technology in this endeavor and there are many opportunities along the process where leveraging parallelism is possible. Meshing, simulation, and post-processing are important examples of&amp;nbsp;these. &lt;/p&gt;
&lt;p&gt;Out of &lt;span class="caps"&gt;CS261&lt;/span&gt; is would like to get insight into what it takes to get extreme-performance out of simulation software in different architectures. In particular, I would like to know more about the following&amp;nbsp;topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="caps"&gt;MPI&lt;/span&gt; usage patterns, how they come up, when to apply them, and&amp;nbsp;tradeoffs.&lt;/li&gt;
&lt;li&gt;Shared memory parallel programming models (pthreads, OpenMP) and recurring software&amp;nbsp;patterns. &lt;/li&gt;
&lt;li&gt;Parallel I/O&amp;nbsp;techniques. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="application-essi-simulator"&gt;Application: &lt;span class="caps"&gt;ESSI&lt;/span&gt;&amp;nbsp;Simulator&lt;/h1&gt;
&lt;p&gt;Introduction&lt;/p&gt;
&lt;h6 id="_1"&gt;&lt;/h6&gt;
&lt;p&gt;The &lt;a href="http://sokocalo.engr.ucdavis.edu/~jeremic/ESSI_Simulator/"&gt;Real &lt;span class="caps"&gt;ESSI&lt;/span&gt; Simulator&lt;/a&gt;[&lt;a href="#essiref"&gt;1&lt;/a&gt; and &lt;a href="#essiref2"&gt;2&lt;/a&gt;]  is a system for simulation of &lt;span class="caps"&gt;ESSI&lt;/span&gt; problems developed at &lt;span class="caps"&gt;UC&lt;/span&gt; Davis. It consists of software, hardware (a parallel computer: &lt;span class="caps"&gt;ESSI&lt;/span&gt; simulator machine), and documentation covering theory, usage and examples for the&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;ESSI&lt;/span&gt; program is a parallel object-oriented finite element analysis (&lt;span class="caps"&gt;FEA&lt;/span&gt;) software for non-linear time domain analysis of &lt;span class="caps"&gt;ESSI&lt;/span&gt; systems. The program is written in C++, using several external libraries to accomplish its goals, most notably &lt;a href="www.open-mpi.org/"&gt;OpenMPI&lt;/a&gt; (message passing interface) is used to achieve parallelism. Other libraries used within &lt;span class="caps"&gt;ESSI&lt;/span&gt; include: &lt;a href="http://www.mcs.anl.gov/petsc/"&gt;PETSc&lt;/a&gt; for parallel solution of system of equations, &lt;a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview"&gt;&lt;span class="caps"&gt;METIS&lt;/span&gt;&lt;/a&gt; and &lt;a href="http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview"&gt;Par-&lt;span class="caps"&gt;METIS&lt;/span&gt;&lt;/a&gt; for graph partitioning, [&lt;span class="caps"&gt;HDF5&lt;/span&gt;]phdf5 for parallel output. Input is controlled by a custom domain-specific language designed specifically for this&amp;nbsp;program. &lt;/p&gt;
&lt;p&gt;The software is meant to target a range of platforms from personal computers (desktop, laptop) to high-performance clusters and&amp;nbsp;supercomputers. &lt;/p&gt;
&lt;p&gt;Parallelism in &lt;span class="caps"&gt;FEA&lt;/span&gt;&lt;/p&gt;
&lt;h6 id="_2"&gt;&lt;/h6&gt;
&lt;p&gt;&lt;img alt="npp" src="/images/other/npp.png" title="Nuclear power plant model and its decomposition." /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Left and middle) Nuclear power plant model showing different areas, (right) domain decomposition of&amp;nbsp;model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Two main sources of parallelism can be identified in the context of nonlinear, dynamic finite element simulation: (i) system of equation solution and (ii) element-level constitutive integration. The first of these consists in the solution of a large linear system of equations (&lt;span class="caps"&gt;SOE&lt;/span&gt;) which arises from the discretization of the continuum problem (expressed as a set of coupled partial differential equations) in the spatial domain. The second source, comes from advancing the constitutive rate equations within each element once a global displacement increment is obtained from the solution of the &lt;span class="caps"&gt;SOE&lt;/span&gt;. This last part can account for a large part of the computational time for large problems and is embarrassingly&amp;nbsp;parallel.&lt;/p&gt;
&lt;p&gt;An additional source of parallelism in &lt;span class="caps"&gt;ESSI&lt;/span&gt; simulations is the storage of the large ammounts of output generated by these simulations. The philosophy adopted by the &lt;span class="caps"&gt;ESSI&lt;/span&gt; simulator is to independently store the information necessary to build the model and restart the simulation at any given point. This gives rise to possibly terabytes of data in even modest models, with the additional problem on how to handle this. In &lt;span class="caps"&gt;ESSI&lt;/span&gt; this is done by using a &lt;a href="http://en.wikipedia.org/wiki/Network_File_System"&gt;network filesystem&lt;/a&gt; (&lt;span class="caps"&gt;NFS&lt;/span&gt;) to create a virtual parallel unique disk and the &lt;span class="caps"&gt;HDF5&lt;/span&gt; format to store the data. In a nutshell, &lt;span class="caps"&gt;HDF5&lt;/span&gt; implements a format for storing scientific (array-oriented) data in a portable way, and also allowing parallel read/write (it uses &lt;span class="caps"&gt;MPI&lt;/span&gt; I/O under the&amp;nbsp;hood).&lt;/p&gt;
&lt;p&gt;A particularity of non-linear (plasticity based) &lt;span class="caps"&gt;FEA&lt;/span&gt; simulation is the unknown parts of the domain may plastify during simulations, leading to increased time spent integrating constitutive equations in that portion of the domain. What this implies is that, given an initial partition that balances the loading, this partition might become unbalanced if the domain plastifies. An adaptation of the dynamic domain decomposition method termed the &amp;#8220;plastic domain decomposition&amp;#8221;[&lt;a href="#pdd"&gt;3&lt;/a&gt;] or &lt;span class="caps"&gt;PDD&lt;/span&gt;, which achieves computational load re-balancing by repartitioning the element graph using computational time as one of the weighting&amp;nbsp;factors.&lt;/p&gt;
&lt;p&gt;Brief design&amp;nbsp;description&lt;/p&gt;
&lt;h6 id="_3"&gt;&lt;/h6&gt;
&lt;p&gt;&lt;span class="caps"&gt;PDD&lt;/span&gt; is implemented in &lt;span class="caps"&gt;ESSI&lt;/span&gt; using the &lt;a href="http://en.wikipedia.org/wiki/Actor_model"&gt;Actor/Shadow&lt;/a&gt; model of concurrency. Actors are autonomous and concurrently executing objects which execute asynchronously. Actors can create new actors and can  send messages to other actors. The Actor model is an Object-Oriented version of message passing in which the Actors represent processes and the methods sent between Actors represent communications (verbatim from [&lt;a href="#lecture_notes"&gt;4&lt;/a&gt;]).&lt;/p&gt;
&lt;p&gt;&lt;img alt="shadowactor" src="/images/other/shadowactor.png" title="Shadow/actor model." /&gt;&lt;/p&gt;
&lt;p&gt;Shadow-actors are the means in which actors communicate. Shadow-actors are objects which represent each running actor (remote) in a particular machine&amp;#8217;s local address space. Shadow actors are in charge of transmitting and receiving messages from other actors and effectively encapsulate all &lt;span class="caps"&gt;MPI&lt;/span&gt; calls. In the object-oriented design of &lt;span class="caps"&gt;ESSI&lt;/span&gt; this model allows reuse of code and modularity when programming using &lt;span class="caps"&gt;MPI&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;Performance&lt;/p&gt;
&lt;h6 id="_4"&gt;&lt;/h6&gt;
&lt;p&gt;As seen in [&lt;a href="#pdd"&gt;3&lt;/a&gt;], plastic-domain-decomposition method provides a viable way to re-balance a plastifying domain and has seen a reasonable scaling for low number of processes. A comprehensive scaling study on different platforms has not been yet&amp;nbsp;performed. &lt;/p&gt;
&lt;p&gt;Currently, &lt;span class="caps"&gt;ESSI&lt;/span&gt;&amp;#8217;s parallelization is exclusively done using &lt;span class="caps"&gt;MPI&lt;/span&gt;. This implies that for the lower end of the platforms we intend to cover (PCs, laptops) there is a performance hit due to improper use of shared memory architecture. This problem could be solved using a mixed design with threads for shared memory nodes and &lt;span class="caps"&gt;MPI&lt;/span&gt; for&amp;nbsp;network. &lt;/p&gt;
&lt;p&gt;Another big bottleneck currently present is that all input is loaded into the master process, partitioned and then distributed. This results in an unnecessary load to the main process at startup and imposes a cap on how big a model might be solved. In order to solve this issue the parser must be parallelized to some extent, so that different (pre-partitioned) model parts can be loaded into different processors on&amp;nbsp;startup. &lt;/p&gt;
&lt;p&gt;&lt;img alt="npp_build" src="/images/other/npp_build.png" title="Different physical zones of the NPP." /&gt;&lt;/p&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a id="essiref"&gt;&lt;/a&gt; Boris Jeremić, Robert Roche-Rivera, Annie Kammerer, Nima Tafazzoli, Jose Abell M., Babak Kamranimoghaddam, Federico Pisano, ChangGyun Jeong and Benjamin Aldridge The &lt;span class="caps"&gt;NRC&lt;/span&gt; &lt;span class="caps"&gt;ESSI&lt;/span&gt; Simulator Program, Current Status in Proceedings of the Structural Mechanics in Reactor Technology (SMiRT) 2013 Conference, San Francisco, August 18-23,&amp;nbsp;2013.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a id="essiref2"&gt;&lt;/a&gt; Boris  Jeremić, Guanzhou Jie, Matthias Preisig and Nima Tafazzoli. Time domain simulation of soil-foundation-structure interaction in non-uniform soils. Earthquake Engineering and Structural Dynamics, Volume 38, Issue 5, pp 699-718,&amp;nbsp;2009.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a id="pdd"&gt;&lt;/a&gt; Boris Jeremić and Guanzhou Jie. Plastic Domain Decomposition Method for Parallel Elastic–Plastic Finite Element Computations in Geomechanics Report &lt;span class="caps"&gt;UCD&lt;/span&gt; CompGeoMech&amp;nbsp;03–2007.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a id="lecture_notes"&gt;&lt;/a&gt; Lecture Notes on Computational Geomechanics: Inelastic Finite Elements for Pressure Sensitive Materials, &lt;span class="caps"&gt;UC&lt;/span&gt; Davis, CompGeoMech&amp;nbsp;group&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jose Abell</dc:creator><pubDate>Thu, 29 Jan 2015 16:57:28 -0800</pubDate><guid>tag:www.joseabell.com,2015-01-29:cs261-hw-0-describe-a-parallel-application.html</guid><category>cs261</category><category>parallel computing</category></item><item><title>Simulation of a surface wave (Rayleigh)</title><link>http://www.joseabell.com/simulation-of-a-surface-wave-English.html</link><description>&lt;p&gt;This simulation, done in the &lt;span class="caps"&gt;UCD&lt;/span&gt; &lt;span class="caps"&gt;ESSI&lt;/span&gt; simulator and visualized in VisIt using VisIt-&lt;span class="caps"&gt;ESSI&lt;/span&gt; plugin, shows the passage of a surface wave (Rayleigh wave). The simulation consists on a surface impact on an elastic domain of 900m by 1800m depth, and a shear wave velocity of 1000km/s. Elliptical-retrograde motion can be seen as an illustration of Rayleigh&amp;nbsp;waves.&lt;/p&gt;
&lt;p&gt;VisIt can be obtained &lt;a href="https://wci.llnl.gov/simulation/computer-codes/visit/"&gt;here&lt;/a&gt;, and the plugin &lt;a href="https://github.com/jaabell/visitESSI"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;!-- more   --&gt;

&lt;!-- https://www.youtube.com/watch?v=mrT5L4xsKs0 --&gt;

&lt;div class="youtube" align="left"&gt;
&lt;iframe width="640" height="480" src="//www.youtube.com/embed/a1xBlIL6ZOM" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jose Abell</dc:creator><pubDate>Sun, 30 Nov 2014 22:42:13 -0800</pubDate><guid>tag:www.joseabell.com,2014-11-30:simulation-of-a-surface-wave-English.html</guid><category>essi</category><category>visitessi</category><category>postprocessing</category><category>movie</category></item><item><title>Visualizing ESSI output with VisIt-ESSI</title><link>http://www.joseabell.com/visualizing-essi-output-with-visit-essi-English.html</link><description>&lt;p&gt;VisIt-&lt;span class="caps"&gt;ESSI&lt;/span&gt; is a plugin for the VisIt post-processor created my CompGeoMech. 
It allows for remote (soon parallel also) visualization of outputs produced
by &lt;span class="caps"&gt;ESSI&lt;/span&gt; in the &lt;span class="caps"&gt;HDF5&lt;/span&gt; format&amp;nbsp;(*.h5.feioutput).&lt;/p&gt;
&lt;p&gt;VisIt can be obtained &lt;a href="https://wci.llnl.gov/simulation/computer-codes/visit/"&gt;here&lt;/a&gt;, and the plugin &lt;a href="https://github.com/jaabell/visitESSI"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;!-- more   --&gt;

&lt;!-- https://www.youtube.com/watch?v=mrT5L4xsKs0 --&gt;

&lt;div class="youtube" align="left"&gt;
&lt;iframe width="640" height="480" src="//www.youtube.com/embed/mrT5L4xsKs0" frameborder="0" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jose Abell</dc:creator><pubDate>Sun, 23 Nov 2014 11:34:38 -0800</pubDate><guid>tag:www.joseabell.com,2014-11-23:visualizing-essi-output-with-visit-essi-English.html</guid><category>tutorial</category><category>essi</category><category>visitessi</category><category>postprocessing</category><category>movie</category></item></channel></rss>